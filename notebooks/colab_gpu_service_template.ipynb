{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e96b01bc",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# Colab GPU Polling Worker\n",
        "Minimal notebook that installs Ollama locally (optional) and runs a background loop that polls your stack’s `/gpu-jobs` queue, executes work on the Colab GPU, and posts results back.\n",
        "\n",
        "> Set these env vars before running:\n",
        "> * `STACK_API_BASE` – HTTPS base URL of your stack (e.g., `https://abc123.ngrok-free.app`)\n",
        "> * `STACK_WORKER_TOKEN` – must match `GPU_WORKER_TOKEN` in `.env`\n",
        "> * Optional: `WORKER_ID`, `POLL_INTERVAL`, `OLLAMA_URL`, `START_OLLAMA`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19a6ff9",
      "metadata": {
        "id": "one_click_setup"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!pip install -q fastapi uvicorn python-multipart requests diffusers accelerate transformers safetensors pillow\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "import textwrap\n",
        "import threading\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "from diffusers import AutoPipelineForText2Image\n",
        "\n",
        "# Optional inline overrides so reruns can tweak values without restarting the runtime.\n",
        "# Leave a value blank (\"\") to keep the environment version, or fill it in to override.\n",
        "ENV_OVERRIDES = {\n",
        "    \"STACK_API_BASE\": \"\",\n",
        "    \"STACK_WORKER_TOKEN\": \"\",\n",
        "    \"WORKER_ID\": \"\",\n",
        "    \"POLL_INTERVAL\": \"\",\n",
        "    \"OLLAMA_PRELOAD_MODELS\": \"\",\n",
        "    \"IMAGEN_MODEL_ID\": \"\",\n",
        "}\n",
        "for _key, _value in ENV_OVERRIDES.items():\n",
        "    if _value:\n",
        "        os.environ[_key] = _value\n",
        "\n",
        "STACK_API_BASE = os.environ.get(\"STACK_API_BASE\")\n",
        "STACK_WORKER_TOKEN = os.environ.get(\"STACK_WORKER_TOKEN\")\n",
        "WORKER_ID = os.environ.get(\"WORKER_ID\", \"colab-worker\")\n",
        "POLL_INTERVAL = float(os.environ.get(\"POLL_INTERVAL\", \"5\"))\n",
        "OLLAMA_URL = os.environ.get(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
        "SERVICE_AUTH_TOKEN = os.environ.get(\"SERVICE_AUTH_TOKEN\", \"colab-shared-12345\")\n",
        "START_OLLAMA = os.environ.get(\"START_OLLAMA\", \"1\") == \"1\"\n",
        "TUNNEL_PORT = int(os.environ.get(\"TUNNEL_PORT\", \"8000\"))  # unused but kept for backwards compat\n",
        "OLLAMA_PRELOAD_MODELS = [m.strip() for m in os.environ.get(\"OLLAMA_PRELOAD_MODELS\", \"llama3:8b\").split(\",\") if m.strip()]\n",
        "IMAGEN_MODEL_ID = os.environ.get(\"IMAGEN_MODEL_ID\", \"runwayml/stable-diffusion-v1-5\")\n",
        "IMAGEN_DEFAULT_STEPS = int(os.environ.get(\"IMAGEN_STEPS\", \"25\"))\n",
        "IMAGEN_MAX_STEPS = int(os.environ.get(\"IMAGEN_MAX_STEPS\", \"50\"))\n",
        "IMAGEN_DEFAULT_WIDTH = int(os.environ.get(\"IMAGEN_WIDTH\", \"512\"))\n",
        "IMAGEN_DEFAULT_HEIGHT = int(os.environ.get(\"IMAGEN_HEIGHT\", \"512\"))\n",
        "IMAGEN_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMAGEN_DTYPE = torch.float16 if IMAGEN_DEVICE == \"cuda\" else torch.float32\n",
        "\n",
        "if not STACK_API_BASE or not STACK_WORKER_TOKEN:\n",
        "    raise ValueError(\"STACK_API_BASE and STACK_WORKER_TOKEN must be set before running this cell.\")\n",
        "\n",
        "# Shut down any previous worker loop spawned by earlier runs of this cell\n",
        "_prev_stop_event = globals().get(\"worker_stop_event\")\n",
        "_prev_worker_thread = globals().get(\"worker_thread\")\n",
        "if _prev_stop_event:\n",
        "    _prev_stop_event.set()\n",
        "if _prev_worker_thread and _prev_worker_thread.is_alive():\n",
        "    _prev_worker_thread.join(timeout=5)\n",
        "\n",
        "globals()[\"worker_stop_event\"] = None\n",
        "globals()[\"worker_thread\"] = None\n",
        "\n",
        "# Launch Ollama serve in the background if desired\n",
        "ollama_proc = None\n",
        "if START_OLLAMA:\n",
        "    ollama_proc = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    # Give the daemon a moment to accept pulls, then ensure requested models are available.\n",
        "    if OLLAMA_PRELOAD_MODELS:\n",
        "        time.sleep(2)\n",
        "        for model_name in OLLAMA_PRELOAD_MODELS:\n",
        "            print(f\"Ensuring Ollama model '{model_name}' is available…\")\n",
        "            try:\n",
        "                subprocess.run([\"ollama\", \"pull\", model_name], check=True)\n",
        "            except subprocess.CalledProcessError as pull_exc:\n",
        "                print(f\"Warning: failed to pull {model_name}: {pull_exc}\")\n",
        "\n",
        "# Simple FastAPI bridge (local only) to expose health + tool endpoints if needed\n",
        "app_py = textwrap.dedent(\n",
        "    \"\"\"\n",
        "import os\n",
        "import httpx\n",
        "from fastapi import FastAPI, Header, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "\n",
        "SERVICE_AUTH_TOKEN = os.environ.get(\"SERVICE_AUTH_TOKEN\", \"colab-shared-12345\")\n",
        "OLLAMA_URL = os.environ.get(\"OLLAMA_URL\", \"http://localhost:11434\")\n",
        "\n",
        "app = FastAPI(title=\"Colab GPU Local Service\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    try:\n",
        "        with httpx.Client(timeout=5.0) as client:\n",
        "            resp = client.get(f\"{OLLAMA_URL}/api/tags\")\n",
        "            resp.raise_for_status()\n",
        "            models = resp.json().get(\"models\", [])\n",
        "    except Exception as exc:  # noqa: BLE001\n",
        "        return JSONResponse({\"status\": \"error\", \"detail\": str(exc)})\n",
        "    return {\"status\": \"ok\", \"models\": models}\n",
        "\n",
        "@app.post(\"/tools/ollamaChat\")\n",
        "def run_ollama_chat(payload: dict, authorization: str = Header(default=\"\")):\n",
        "    expected = f\"Bearer {SERVICE_AUTH_TOKEN}\"\n",
        "    if authorization != expected:\n",
        "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
        "    with httpx.Client(timeout=120.0) as client:\n",
        "        resp = client.post(f\"{OLLAMA_URL}/api/generate\", json=payload, stream=False)\n",
        "        resp.raise_for_status()\n",
        "        return resp.json()\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "Path(\"app.py\").write_text(app_py)\n",
        "\n",
        "server = subprocess.Popen(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"uvicorn\",\n",
        "        \"app:app\",\n",
        "        \"--host\",\n",
        "        \"0.0.0.0\",\n",
        "        f\"--port={TUNNEL_PORT}\",\n",
        "    ],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "print(\"Local FastAPI server PID:\", server.pid)\n",
        "print(\"STACK_API_BASE:\", STACK_API_BASE)\n",
        "print(\"Worker ID:\", WORKER_ID)\n",
        "\n",
        "\n",
        "def image_to_base64(image):\n",
        "    buf = io.BytesIO()\n",
        "    image.save(buf, format=\"PNG\")\n",
        "    return \"data:image/png;base64,\" + base64.b64encode(buf.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def clamp_steps(value, fallback, max_value):\n",
        "    try:\n",
        "        parsed = int(value)\n",
        "    except (TypeError, ValueError):\n",
        "        parsed = fallback\n",
        "    parsed = max(5, min(parsed, max_value))\n",
        "    return parsed\n",
        "\n",
        "\n",
        "def multiple_of_eight(value, fallback):\n",
        "    try:\n",
        "        parsed = int(value)\n",
        "    except (TypeError, ValueError):\n",
        "        parsed = fallback\n",
        "    parsed = max(256, min(parsed, 1024))\n",
        "    parsed -= parsed % 8\n",
        "    return parsed or fallback\n",
        "\n",
        "\n",
        "diffusers_lock = threading.Lock()\n",
        "diffusers_pipeline = None\n",
        "\n",
        "\n",
        "def ensure_diffusers_pipeline():\n",
        "    global diffusers_pipeline\n",
        "    with diffusers_lock:\n",
        "        if diffusers_pipeline is None:\n",
        "            pipe = AutoPipelineForText2Image.from_pretrained(IMAGEN_MODEL_ID, torch_dtype=IMAGEN_DTYPE)\n",
        "            pipe = pipe.to(IMAGEN_DEVICE)\n",
        "            pipe.safety_checker = None\n",
        "            diffusers_pipeline = pipe\n",
        "    return diffusers_pipeline\n",
        "\n",
        "\n",
        "def run_imagen_job(job_payload):\n",
        "    body = job_payload.get(\"payload\") or {}\n",
        "    prompt = body.get(\"prompt\")\n",
        "    if not isinstance(prompt, str) or not prompt.strip():\n",
        "        raise ValueError(\"prompt is required for imagenGenerate\")\n",
        "\n",
        "    negative_prompt = body.get(\"negative_prompt\")\n",
        "    guidance_scale = float(body.get(\"guidance_scale\", 7.0))\n",
        "    steps = clamp_steps(body.get(\"num_inference_steps\"), IMAGEN_DEFAULT_STEPS, IMAGEN_MAX_STEPS)\n",
        "    width = multiple_of_eight(body.get(\"width\"), IMAGEN_DEFAULT_WIDTH)\n",
        "    height = multiple_of_eight(body.get(\"height\"), IMAGEN_DEFAULT_HEIGHT)\n",
        "    seed = body.get(\"seed\")\n",
        "\n",
        "    pipeline = ensure_diffusers_pipeline()\n",
        "    generator = None\n",
        "    used_seed = None\n",
        "    if seed is not None:\n",
        "        try:\n",
        "            used_seed = int(seed)\n",
        "            generator = torch.Generator(device=IMAGEN_DEVICE).manual_seed(used_seed)\n",
        "        except (TypeError, ValueError):\n",
        "            used_seed = None\n",
        "\n",
        "    started = time.time()\n",
        "    result = pipeline(\n",
        "        prompt=prompt.strip(),\n",
        "        negative_prompt=negative_prompt.strip() if isinstance(negative_prompt, str) else None,\n",
        "        guidance_scale=guidance_scale,\n",
        "        num_inference_steps=steps,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        generator=generator,\n",
        "    )\n",
        "    duration_ms = int((time.time() - started) * 1000)\n",
        "    image = result.images[0]\n",
        "\n",
        "    return {\n",
        "        \"image_base64\": image_to_base64(image),\n",
        "        \"model\": IMAGEN_MODEL_ID,\n",
        "        \"prompt\": prompt,\n",
        "        \"negative_prompt\": negative_prompt,\n",
        "        \"guidance_scale\": guidance_scale,\n",
        "        \"num_inference_steps\": steps,\n",
        "        \"width\": width,\n",
        "        \"height\": height,\n",
        "        \"seed\": used_seed,\n",
        "        \"duration_ms\": duration_ms,\n",
        "        \"device\": IMAGEN_DEVICE,\n",
        "    }\n",
        "\n",
        "\n",
        "def run_gpu_job(job_payload):\n",
        "    tool = job_payload.get(\"tool\")\n",
        "    body = job_payload.get(\"payload\") or {}\n",
        "    body.setdefault(\"stream\", False)\n",
        "    if tool == \"ollamaChat\":\n",
        "        response = requests.post(f\"{OLLAMA_URL}/api/generate\", json=body, timeout=180)\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    if tool == \"imagenGenerate\":\n",
        "        return run_imagen_job(job_payload)\n",
        "    raise ValueError(f\"Unsupported tool: {tool}\")\n",
        "\n",
        "\n",
        "def start_worker_loop():\n",
        "    base = STACK_API_BASE.rstrip(\"/\")\n",
        "    headers = {\"Authorization\": f\"Bearer {STACK_WORKER_TOKEN}\"}\n",
        "    stop_event = threading.Event()\n",
        "\n",
        "    def poll_forever():\n",
        "        print(f\"Worker loop started for {WORKER_ID}; polling {base} every {POLL_INTERVAL}s\")\n",
        "        while not stop_event.is_set():\n",
        "            try:\n",
        "                resp = requests.get(\n",
        "                    f\"{base}/gpu-jobs/next\",\n",
        "                    params={\"worker\": WORKER_ID},\n",
        "                    headers=headers,\n",
        "                    timeout=30,\n",
        "                )\n",
        "                if resp.status_code == 204:\n",
        "                    time.sleep(POLL_INTERVAL)\n",
        "                    continue\n",
        "                resp.raise_for_status()\n",
        "                job = resp.json().get(\"job\")\n",
        "                if not job:\n",
        "                    time.sleep(POLL_INTERVAL)\n",
        "                    continue\n",
        "                job_id = job[\"id\"]\n",
        "                job_tool = job.get(\"tool\", \"unknown\")\n",
        "                print(f\"[worker] Leased job {job_id} ({job_tool}) at {time.strftime('%H:%M:%S')}\")\n",
        "                try:\n",
        "                    result = run_gpu_job(job)\n",
        "                    requests.post(\n",
        "                        f\"{base}/gpu-jobs/{job_id}/complete\",\n",
        "                        headers=headers,\n",
        "                        json={\"status\": \"completed\", \"result\": result},\n",
        "                        timeout=30,\n",
        "                    )\n",
        "                    print(f\"[worker] Completed job {job_id}\")\n",
        "                except Exception as job_exc:  # noqa: BLE001\n",
        "                    print(f\"[worker] Job failed {job_id}: {job_exc}\")\n",
        "                    requests.post(\n",
        "                        f\"{base}/gpu-jobs/{job_id}/complete\",\n",
        "                        headers=headers,\n",
        "                        json={\"status\": \"error\", \"detail\": str(job_exc)},\n",
        "                        timeout=30,\n",
        "                    )\n",
        "            except requests.HTTPError as http_exc:\n",
        "                if http_exc.response is not None and http_exc.response.status_code == 401:\n",
        "                    print(\"Worker auth failed; stopping loop.\")\n",
        "                    break\n",
        "                print(\"Polling HTTP error:\", http_exc)\n",
        "                time.sleep(POLL_INTERVAL)\n",
        "            except Exception as exc:  # noqa: BLE001\n",
        "                print(\"Polling error:\", exc)\n",
        "                time.sleep(POLL_INTERVAL)\n",
        "        print(\"Worker loop exiting…\")\n",
        "\n",
        "    thread = threading.Thread(target=poll_forever, name=\"gpu-worker\", daemon=True)\n",
        "    thread.start()\n",
        "\n",
        "    globals()[\"worker_stop_event\"] = stop_event\n",
        "    globals()[\"worker_thread\"] = thread\n",
        "\n",
        "\n",
        "start_worker_loop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54263387",
      "metadata": {},
      "source": [
        "## Usage\n",
        "1. Set `STACK_API_BASE` and `STACK_WORKER_TOKEN` (match `.env` → `GPU_WORKER_TOKEN`).\n",
        "2. Run the setup cell once; it installs Ollama (optional), starts a local FastAPI bridge, and begins polling `/gpu-jobs` on your stack.\n",
        "3. Submit jobs via `POST /gpu-jobs` on the stack; the worker picks them up automatically and reports completion via `/gpu-jobs/{id}/complete`.\n",
        "4. Use the cleanup cell to terminate the local server (optional if the runtime is about to reset).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0133f0b6",
      "metadata": {
        "id": "cleanup"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "if \"public_tunnel\" in globals():\n",
        "    try:\n",
        "        ngrok.disconnect(public_tunnel.public_url)\n",
        "        print(\"ngrok tunnel closed\")\n",
        "    except Exception as exc:\n",
        "        print(\"ngrok cleanup warning:\", exc)\n",
        "else:\n",
        "    print(\"No tunnel to close\")\n",
        "\n",
        "if \"server\" in globals():\n",
        "    try:\n",
        "        server.terminate()\n",
        "        print(\"Server stopped\")\n",
        "    except Exception as exc:\n",
        "        print(\"Server cleanup warning:\", exc)\n",
        "else:\n",
        "    print(\"No server process found\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
