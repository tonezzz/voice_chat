services:
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - "C:/_dev/_models/ollama:/root/.ollama"
    restart: unless-stopped

  ollama-gpu:
    image: ollama/ollama
    container_name: ollama-gpu
    ports:
      - "11435:11434"
    volumes:
      - "C:/_dev/_models/ollama:/root/.ollama"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    runtime: nvidia
    restart: unless-stopped

  stt:
    build: ./stt
    container_name: stt-whisper
    environment:
      - WHISPER_MODEL=tiny
      - WHISPER_DEVICE=cpu
      - WHISPER_DOWNLOAD_DIR=/app/models
    volumes:
      - C:/_dev/_models/huggingface/stt:/app/models
    ports:
      - "5001:5001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  stt-gpu:
    build: ./stt
    container_name: stt-whisper-gpu
    environment:
      - WHISPER_MODEL=base
      - WHISPER_DEVICE=cuda
      - WHISPER_DOWNLOAD_DIR=/app/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    ports:
      - "5002:5001"
    volumes:
      - C:/_dev/_models/huggingface/stt:/app/models
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  server:
    build: ./server
    container_name: voice-chat-server
    environment:
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_GPU_URL=http://ollama-gpu:11434
      - MODEL=llama3.2-vision:11b
      - PORT=3001
      - STT_URL=http://stt:5001
      - STT_GPU_URL=http://stt-gpu:5001
      - TTS_ENABLED=true
      - TTS_URL=http://tts:59125
      - TTS_GPU_URL=http://tts-gpu:59125
      - TTS_VOICE=
      - YOLO_MCP_URL=http://yolo_mcp:8000
    ports:
      - "3002:3001"
    depends_on:
      - ollama
      - stt
      - ollama-gpu
      - stt-gpu
      - tts
      - tts-gpu
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  tts:
    build: ./tts
    container_name: voice-chat-tts
    environment:
      - PIPER_MODEL_URL=https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US-amy-medium.onnx
      - PIPER_CONFIG_URL=https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US-amy-medium.onnx.json
    volumes:
      - C:/_dev/_models/huggingface/tts:/app/models
    ports:
      - "59125:59125"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:59125/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  tts-gpu:
    build: ./tts
    container_name: voice-chat-tts-gpu
    environment:
      - PIPER_MODEL_URL=https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US-amy-medium.onnx
      - PIPER_CONFIG_URL=https://huggingface.co/rhasspy/piper-voices/resolve/main/en/en_US-amy-medium.onnx.json
      - PIPER_SAMPLE_RATE=22050
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    volumes:
      - tts-models:/app/models
    ports:
      - "59126:59125"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:59125/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  ngrok:
    image: ngrok/ngrok:latest
    container_name: voice-chat-ngrok
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    command:
      - http
      - server:3001
    depends_on:
      - server
    restart: unless-stopped

  yolo_mcp:
    build: ./yolo_mcp
    container_name: yolo-mcp
    environment:
      - YOLO_MODEL=yolov8n.pt
    volumes:
      - C:/_dev/_models/yolo:/app/models
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

volumes:
  ollama-data:
  tts-models:
