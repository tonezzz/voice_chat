services:
  # openvoice-tts:
  #   build: C:/_dev/_models/openvoice/openvoice
  #   container_name: voice-chat-openvoice
  #   profiles: ["cpu"]
  #   environment:
  #     - OPENVOICE_V2_DIR=/app/inference/openvoice_v2
  #     - OPENVOICE_CHECKPOINT_DIR=/app/inference/checkpoints
  #   volumes:
  #     - C:/_dev/_models/openvoice_v2:/app/inference/openvoice_v2
  #     - C:/_dev/_models/openvoice/data:/app/inference/.data
  #     - C:/_dev/_models/openvoice/checkpoints:/app/inference/checkpoints
  #     - C:/_dev/_models/openvoice/references:/app/inference/references
  #     - C:/_dev/_models/openvoice/conf/openvoice:/app/conf/openvoice
  #     #- ./server/conf/openvoice:/app/conf/openvoice
  #   ports:
  #     - "8100:80"
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -sf http://localhost/hc || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #   restart: unless-stopped

  mcp_bslip:
    build: ./mcp-bslip
    container_name: mcp-bslip
    environment:
      - PORT=8002
    ports:
      - "8002:8002"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8002/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  mcp_idp:
    build: ./mcp_idp
    container_name: mcp-idp
    profiles: ["cpu"]
    environment:
      - IDP_DEVICE=${IDP_DEVICE:-cpu}
      - IDP_REFERENCE_DIR=/app/reference_faces
      - IDP_MODEL_STORAGE=/app/models
      - PORT=8004
    volumes:
      - ${IDP_REFERENCE_ROOT:-C:/_dev/_models/idp/references}:/app/reference_faces
      - ${IDP_MODEL_ROOT:-C:/_dev/_models/idp/models}:/app/models
    ports:
      - "8004:8004"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8004/health || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 5
    restart: unless-stopped

  mcp_idp_gpu:
    build:
      context: ./mcp_idp
      args:
        BASE_IMAGE: ${CUDA_BASE_IMAGE:-nvidia/cuda:12.4.1-runtime-ubuntu22.04}
        USE_GPU: "true"
    container_name: mcp-idp-gpu
    profiles: ["gpu"]
    environment:
      - IDP_DEVICE=cuda
      - IDP_REFERENCE_DIR=/app/reference_faces
      - IDP_MODEL_STORAGE=/app/models
      - PORT=8004
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    volumes:
      - ${IDP_REFERENCE_ROOT:-C:/_dev/_models/idp/references}:/app/reference_faces
      - ${IDP_MODEL_ROOT:-C:/_dev/_models/idp/models}:/app/models
    ports:
      - "8104:8004"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8004/health || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 5
    restart: unless-stopped

  mcp_0:
    build: ./mcp_0
    container_name: mcp-0
    profiles: ["cpu", "gpu"]
    init: true
    environment:
      - MCP0_HOST=0.0.0.0
      - MCP0_PORT=8010
      - MCP0_TIMEOUT_SECONDS=10
      - GITHUB_MCP_URL=${GITHUB_MCP_URL:-http://mcp-github:8080}
      - GITHUB_MCP_HEALTH_PATH=${GITHUB_MCP_HEALTH_PATH:-/health}
      - GITHUB_MCP_TOOLS=${GITHUB_MCP_TOOLS:-list_models+run_space}
      - GITHUB_MCP_TOKEN=${GITHUB_MCP_TOKEN:-}
      - GITHUB_PERSONAL_TOKEN=${GITHUB_PERSONAL_TOKEN:-}
      - MCP0_PROVIDERS=yolo:http://mcp-yolo:8000|health=/|capabilities=|tools=detect_objects+detect_http,bslip:http://mcp-bslip:8002|health=/health|capabilities=|tools=verify_slip,image:http://mcp-imagen:8001|health=/|capabilities=|tools=generate_image+generate_stream,image_gpu:http://mcp-imagen-gpu:8001|health=/|capabilities=|tools=generate_image+generate_stream,idp:http://mcp-idp:8004|health=/health|capabilities=|tools=identify_people+list_identity_references+refresh_identity_references,idp_gpu:http://mcp-idp-gpu:8004|health=/health|capabilities=|tools=identify_people+list_identity_references+refresh_identity_references,github:${GITHUB_MCP_URL:-http://mcp-github:8080}|health=${GITHUB_MCP_HEALTH_PATH:-/health}|health_method=GET|capabilities=/.well-known/mcp.json|tools=${GITHUB_MCP_TOOLS:-list_models+run_space}
    ports:
      - "8010:8010"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8010/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  mcp-github:
    build: ./mcp_github_bridge
    container_name: mcp-github
    profiles: ["cpu", "gpu"]
    environment:
      - GITHUB_PERSONAL_TOKEN=${GITHUB_PERSONAL_TOKEN:-}
      - GITHUB_PERSONAL_ACCESS_TOKEN=${GITHUB_PERSONAL_ACCESS_TOKEN:-}
      - GITHUB_TOOLSETS=${GITHUB_TOOLSETS:-default}
      - LOG_LEVEL=${GITHUB_MCP_BRIDGE_LOG_LEVEL:-INFO}
    ports:
      - "8200:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
    restart: unless-stopped

  openvoice-tts-gpu:
    build: C:/_dev/_models/openvoice/openvoice
    container_name: voice-chat-openvoice-gpu
    profiles: ["gpu"]
    environment:
      - OPENVOICE_V2_DIR=/app/inference/openvoice_v2
      - NVIDIA_VISIBLE_DEVICES=all
      - OPENVOICE_CHECKPOINT_DIR=/app/inference/checkpoints
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    volumes:
      - C:/_dev/_models/openvoice_v2:/app/inference/openvoice_v2
      - C:/_dev/_models/openvoice/data:/app/inference/.data
      - C:/_dev/_models/openvoice/checkpoints:/app/inference/checkpoints
      - C:/_dev/_models/openvoice/references:/app/inference/references
      - C:/_dev/_models/openvoice/conf/openvoice:/app/conf/openvoice
    ports:
      - "8101:80"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost/hc || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
  ollama:
    image: ollama/ollama
    container_name: ollama
    profiles: ["cpu"]
    ports:
      - "11434:11434"
    volumes:
      - "C:/_dev/_models/ollama:/root/.ollama"
    restart: unless-stopped

  ollama-gpu:
    image: ollama/ollama
    container_name: ollama-gpu
    profiles: ["gpu"]
    ports:
      - "11435:11434"
    volumes:
      - "C:/_dev/_models/ollama:/root/.ollama"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    runtime: nvidia
    restart: unless-stopped

  stt:
    build: ./stt
    container_name: stt-whisper
    profiles: ["cpu"]
    environment:
      - WHISPER_MODEL=tiny
      - WHISPER_DEVICE=cpu
      - WHISPER_DOWNLOAD_DIR=/app/models
    volumes:
      - C:/_dev/_models/huggingface/stt:/app/models
    ports:
      - "5001:5001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  stt-gpu:
    build: ./stt
    container_name: stt-whisper-gpu
    profiles: ["gpu"]
    init: true
    environment:
      - WHISPER_MODEL=base
      - WHISPER_DEVICE=cuda
      - WHISPER_DOWNLOAD_DIR=/app/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    ports:
      - "5002:5001"
    volumes:
      - ${HF_STT_MODELS:-C:/_dev/_models/huggingface/stt}:/app/models
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  server:
    build: ./server
    container_name: voice-chat-server
    init: true
    environment:
      - NODE_ENV=production
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_GPU_URL=http://ollama-gpu:11434
      - MODEL=${OLLAMA_MODEL:-llama3.2:3b}
      - PORT=3001
      - STT_URL=http://stt:5001
      - STT_GPU_URL=http://stt-gpu:5001
      - OPENVOICE_GPU_URL=http://openvoice-tts-gpu:80
      - YOLO_MCP_URL=http://mcp-yolo:8000
      - BSLIP_MCP_URL=http://mcp-bslip:8002
      - IMAGE_MCP_URL=http://mcp-imagen:8001
      - IMAGE_MCP_GPU_URL=http://mcp-imagen-gpu:8001
      - IDP_MCP_URL=http://mcp-idp:8004
      - IDP_MCP_GPU_URL=http://mcp-idp-gpu:8004
      - MCP0_URL=http://mcp-0:8010
      - GITHUB_MCP_URL=${GITHUB_MCP_URL:-http://mcp-github:8080}
      - GITHUB_MCP_HEALTH_PATH=${GITHUB_MCP_HEALTH_PATH:-/health}
      - GITHUB_MCP_TOKEN=${GITHUB_MCP_TOKEN:-}
      - GITHUB_PERSONAL_TOKEN=${GITHUB_PERSONAL_TOKEN:-}
    ports:
      - "3002:3001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped


  ngrok:
    image: ngrok/ngrok:latest
    container_name: voice-chat-ngrok
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    command:
      - http
      - server:3001
    depends_on:
      - server
    restart: unless-stopped

  mcp_yolo:
    build: ./mcp_yolo
    container_name: mcp-yolo
    environment:
      - YOLO_MODEL=yolov8n.pt
    volumes:
      - C:/_dev/_models/yolo:/app/models
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  mcp-imagen:
    build: ./mcp-imagen
    container_name: mcp-imagen
    profiles: ["cpu"]
    environment:
      - IMAGE_MODEL_ID=${IMAGE_MODEL_ID:-runwayml/stable-diffusion-v1-5}
      - TORCH_DEVICE=${IMAGE_TORCH_DEVICE:-cpu}
      - IMAGE_STEPS=${IMAGE_STEPS:-25}
      - IMAGE_MAX_STEPS=${IMAGE_MAX_STEPS:-60}
      - IMAGE_WIDTH=${IMAGE_WIDTH:-512}
      - IMAGE_HEIGHT=${IMAGE_HEIGHT:-512}
      - IMAGE_MODEL_CACHE=/app/models
    volumes:
      - C:/_dev/_models/diffusers:/app/models
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/ || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 5
    restart: unless-stopped

  mcp-imagen-gpu:
    build:
      context: ./mcp-imagen
      args:
        BASE_IMAGE: ${CUDA_BASE_IMAGE:-nvidia/cuda:12.4.1-runtime-ubuntu22.04}
        USE_GPU: "true"
    container_name: mcp-imagen-gpu
    profiles: ["gpu"]
    environment:
      - IMAGE_MODEL_ID=${IMAGE_MODEL_ID:-runwayml/stable-diffusion-v1-5}
      - TORCH_DEVICE=${IMAGE_TORCH_DEVICE_GPU:-cuda}
      - IMAGE_STEPS=${IMAGE_STEPS:-25}
      - IMAGE_MAX_STEPS=${IMAGE_MAX_STEPS:-60}
      - IMAGE_WIDTH=${IMAGE_WIDTH:-512}
      - IMAGE_HEIGHT=${IMAGE_HEIGHT:-512}
      - IMAGE_MODEL_CACHE=/app/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    volumes:
      - ${IMAGE_MODEL_ROOT:-C:/_dev/_models/diffusers}:/app/models
    ports:
      - "8102:8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/ || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 5
    restart: unless-stopped

volumes:
  ollama-data:
