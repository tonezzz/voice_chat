services:
  openvoice-tts:
    build: C:/_dev/_models/openvoice/openvoice
    container_name: voice-chat-openvoice
    environment:
      - OPENVOICE_V2_DIR=/app/inference/openvoice_v2
      - OPENVOICE_CHECKPOINT_DIR=/app/inference/checkpoints
    volumes:
      - C:/_dev/_models/openvoice_v2:/app/inference/openvoice_v2
      - C:/_dev/_models/openvoice/data:/app/inference/.data
      - C:/_dev/_models/openvoice/checkpoints:/app/inference/checkpoints
      - C:/_dev/_models/openvoice/references:/app/inference/references
      - C:/_dev/_models/openvoice/conf/openvoice:/app/conf/openvoice
      #- ./server/conf/openvoice:/app/conf/openvoice
    ports:
      - "8100:80"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost/hc || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  openvoice-tts-gpu:
    build: C:/_dev/_models/openvoice/openvoice
    container_name: voice-chat-openvoice-gpu
    environment:
      - OPENVOICE_V2_DIR=/app/inference/openvoice_v2
      - NVIDIA_VISIBLE_DEVICES=all
      - OPENVOICE_CHECKPOINT_DIR=/app/inference/checkpoints
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    volumes:
      - C:/_dev/_models/openvoice_v2:/app/inference/openvoice_v2
      - C:/_dev/_models/openvoice/data:/app/inference/.data
      - C:/_dev/_models/openvoice/checkpoints:/app/inference/checkpoints
      - C:/_dev/_models/openvoice/references:/app/inference/references
      - C:/_dev/_models/openvoice/conf/openvoice:/app/conf/openvoice
    ports:
      - "8101:80"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost/hc || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - "C:/_dev/_models/ollama:/root/.ollama"
    restart: unless-stopped

  ollama-gpu:
    image: ollama/ollama
    container_name: ollama-gpu
    ports:
      - "11435:11434"
    volumes:
      - "C:/_dev/_models/ollama:/root/.ollama"
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    environment:
      - OLLAMA_KEEP_ALIVE=5m
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    runtime: nvidia
    restart: unless-stopped

  stt:
    build: ./stt
    container_name: stt-whisper
    environment:
      - WHISPER_MODEL=tiny
      - WHISPER_DEVICE=cpu
      - WHISPER_DOWNLOAD_DIR=/app/models
    volumes:
      - C:/_dev/_models/huggingface/stt:/app/models
    ports:
      - "5001:5001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  stt-gpu:
    build: ./stt
    container_name: stt-whisper-gpu
    environment:
      - WHISPER_MODEL=base
      - WHISPER_DEVICE=cuda
      - WHISPER_DOWNLOAD_DIR=/app/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    ports:
      - "5002:5001"
    volumes:
      - ${HF_STT_MODELS:-C:/_dev/_models/huggingface/stt}:/app/models
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:5001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  server:
    build: ./server
    container_name: voice-chat-server
    environment:
      - NODE_ENV=production
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_GPU_URL=http://ollama-gpu:11434
      - MODEL=${OLLAMA_MODEL:-llama3.2:3b}
      - PORT=3001
      - STT_URL=http://stt:5001
      - STT_GPU_URL=http://stt:5001
      - OPENVOICE_URL=http://openvoice-tts:80
      - OPENVOICE_GPU_URL=http://openvoice-tts-gpu:80
      - YOLO_MCP_URL=http://yolo-mcp:8000
      - IMAGE_MCP_URL=http://image-mcp:8001
      - IMAGE_MCP_GPU_URL=http://image-mcp-gpu:8001
    ports:
      - "3002:3001"
    depends_on:
      - ollama
      - stt
      - yolo_mcp
      - image_mcp
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:3001/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped


  ngrok:
    image: ngrok/ngrok:latest
    container_name: voice-chat-ngrok
    environment:
      - NGROK_AUTHTOKEN=${NGROK_AUTHTOKEN}
    command:
      - http
      - server:3001
    depends_on:
      - server
    restart: unless-stopped

  yolo_mcp:
    build: ./yolo_mcp
    container_name: yolo-mcp
    environment:
      - YOLO_MODEL=yolov8n.pt
    volumes:
      - C:/_dev/_models/yolo:/app/models
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  image_mcp:
    build: ./image_mcp
    container_name: image-mcp
    environment:
      - IMAGE_MODEL_ID=${IMAGE_MODEL_ID:-runwayml/stable-diffusion-v1-5}
      - TORCH_DEVICE=${IMAGE_TORCH_DEVICE:-cpu}
      - IMAGE_STEPS=${IMAGE_STEPS:-25}
      - IMAGE_MAX_STEPS=${IMAGE_MAX_STEPS:-60}
      - IMAGE_WIDTH=${IMAGE_WIDTH:-512}
      - IMAGE_HEIGHT=${IMAGE_HEIGHT:-512}
      - IMAGE_MODEL_CACHE=/app/models
    volumes:
      - C:/_dev/_models/diffusers:/app/models
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/ || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 5
    restart: unless-stopped

  image_mcp_gpu:
    build:
      context: ./image_mcp
      args:
        BASE_IMAGE: ${CUDA_BASE_IMAGE:-nvidia/cuda:12.4.1-runtime-ubuntu22.04}
        USE_GPU: "true"
    container_name: image-mcp-gpu
    environment:
      - IMAGE_MODEL_ID=${IMAGE_MODEL_ID:-runwayml/stable-diffusion-v1-5}
      - TORCH_DEVICE=${IMAGE_TORCH_DEVICE_GPU:-cuda}
      - IMAGE_STEPS=${IMAGE_STEPS:-25}
      - IMAGE_MAX_STEPS=${IMAGE_MAX_STEPS:-60}
      - IMAGE_WIDTH=${IMAGE_WIDTH:-512}
      - IMAGE_HEIGHT=${IMAGE_HEIGHT:-512}
      - IMAGE_MODEL_CACHE=/app/models
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    runtime: nvidia
    volumes:
      - ${IMAGE_MODEL_ROOT:-C:/_dev/_models/diffusers}:/app/models
    ports:
      - "8102:8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8001/ || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 5
    restart: unless-stopped

volumes:
  ollama-data:
